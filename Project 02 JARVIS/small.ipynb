{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Load the pre-trained English language model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Define the text to analyze\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis is a sample text to analyze using spaCy.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"This is a sample text to analyze using spaCy.\"\n",
    "\n",
    "# Analyze the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print out the tokens and their POS tags\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  \n",
      "quick  \n",
      "brown  \n",
      "fox  \n",
      "jumps  \n",
      "over  \n",
      "the  \n",
      "lazy  \n",
      "dog  \n",
      ".  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load the English tokenizer, tagger, parser and NER models\n",
    "nlp = English()\n",
    "\n",
    "# Define a sentence\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the tokens in the Doc object\n",
    "for token in doc:\n",
    "    print(token.text,  token.pos_, token.dep_)\n",
    "\n",
    "# Output:\n",
    "# The DET det\n",
    "# quick ADJ amod\n",
    "# brown ADJ amod\n",
    "# fox NOUN nsubj\n",
    "# jumps VERB ROOT\n",
    "# over ADP prep\n",
    "# the DET det\n",
    "# lazy ADJ amod\n",
    "# dog NOUN pobj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m ner \u001b[39m=\u001b[39m EntityRecognizer(nlp\u001b[39m.\u001b[39mvocab,[\u001b[39m'\u001b[39m\u001b[39mANIMAL\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[39m# Add the food entity to the entity recognizer\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m ner\u001b[39m.\u001b[39;49madd_label(\u001b[39m\"\u001b[39;49m\u001b[39mFOOD\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     30\u001b[0m \u001b[39m# Train the entity recognizer\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m _, annotations \u001b[39min\u001b[39;00m TRAIN_DATA:\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:172\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.add_label\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:195\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser._resize\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'attrs'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities \t []\n",
      "Noun Pharases \t ['jarvis', 'the music']\n",
      "Verbs    \t ['open']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function to process text input\n",
    "def process_input(text):\n",
    "    # Parse the input using the NLP model\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities from the input\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "    \n",
    "    # Extract noun phrases from the input\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    # Extract verbs from the input\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    # Return the extracted features\n",
    "    return { \"Named Entities\" : named_entities,\"Noun Pharases\" : noun_phrases,\"Verbs   \" : verbs } \n",
    "\n",
    "\n",
    "\n",
    "strr = \"jarvis open the music\"\n",
    "\n",
    "returned_dict = process_input(strr)\n",
    "\n",
    "for key in returned_dict:\n",
    "    print(f\"{key} \\t {returned_dict[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment \t Positive\n",
      "sentiment scoreA \t {'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'compound': 0.3182}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#sentiment anlysic\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the sentiment analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    # Analyze the sentiment of the text using the sentiment analyzer\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Determine the overall sentiment based on the compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    # Return the sentiment and scores\n",
    "    return { \"sentiment\" : sentiment,\"sentiment scoreA\" : sentiment_scores }\n",
    "\n",
    "\n",
    "strr = \"can you please open the window.\"\n",
    "returned_dict  = analyze_sentiment(strr)\n",
    "\n",
    "for key in returned_dict:\n",
    "      print(f\"{key} \\t {returned_dict[key]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please pass the salt. \t Declarative\n",
      "What time is it? \t Interrogative\n",
      "She sings beautifully. \t Declarative\n",
      "The cat sat on the mat. \t Declarative\n",
      "Why are you crying? \t Interrogative\n",
      "Take out the trash. \t Imperative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ritik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Types of sentances\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a function to determine the type of a sentence\n",
    "def determine_sentence_type(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Determine the part-of-speech tags for each word\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Determine the type of the sentence based on the first word and the part-of-speech tags\n",
    "    first_word, first_tag = pos_tags[0]\n",
    "    \n",
    "    if first_tag == 'VB':\n",
    "        sentence_type = 'Imperative'\n",
    "    elif first_tag == 'RB':\n",
    "        sentence_type = 'Interrogative'\n",
    "    elif first_word in ['What', 'Where', 'When', 'Who', 'Why', 'How']:\n",
    "        sentence_type = 'Interrogative'\n",
    "    else:\n",
    "        sentence_type = 'Declarative'\n",
    "    \n",
    "    return sentence_type\n",
    "\n",
    "# Test the function with example sentences\n",
    "sentences = [\n",
    "    \"Please pass the salt.\",\n",
    "    \"What time is it?\",\n",
    "    \"She sings beautifully.\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Why are you crying?\",\n",
    "    \"Take out the trash.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_type = determine_sentence_type(sentence)\n",
    "    print(f\"{sentence} \\t {sentence_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you are running this command in a jupyter notebook, it opens another window titled 'NLTK Downloader' Once you go in that window, you can select the topics you want to download and then click on download button to start downloading.\n"
     ]
    }
   ],
   "source": [
    "# text summarizatiion\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def summarize_text(text, num_sentences):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words from the words list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.casefold() not in stop_words]\n",
    "    \n",
    "    # Create a frequency distribution of the words\n",
    "    freq_dist = nltk.FreqDist(words)\n",
    "    \n",
    "    # Determine the most frequent words\n",
    "    most_freq_words = [word for word, freq in freq_dist.most_common(20)]\n",
    "    \n",
    "    # Create a list of sentences that contain the most frequent words\n",
    "    summary_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(word in sentence for word in most_freq_words):\n",
    "            summary_sentences.append(sentence)\n",
    "    \n",
    "    # Sort the summary sentences by their order in the original text\n",
    "    summary_sentences = sorted(summary_sentences, key=sentences.index)\n",
    "    \n",
    "    # Return the first num_sentences of the summary sentences\n",
    "    summary = ' '.join(summary_sentences[:num_sentences])\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "If you are running this command in a jupyter notebook, it opens another window titled 'NLTK Downloader' Once you go in that window, you can select the topics you want to download and then click on download button to start downloading.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(text, 1)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you said: back\n",
      "I think you said: back\n"
     ]
    }
   ],
   "source": [
    "# supervised ai model pychon code using nlp to listem instructions.\n",
    "# Here's a Python program that demonstrates how to build a supervised AI model using NLP to listen to instructions:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and punctuation\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # Join the words back into a string and return it\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Define the training data\n",
    "train_data = [\n",
    "    {'text': 'move left', 'label': 'left'},\n",
    "    {'text': 'go right', 'label': 'right'},\n",
    "    {'text': 'turn around', 'label': 'turn'},\n",
    "    {'text': 'walk forward', 'label': 'forward'},\n",
    "    {'text': 'step back', 'label': 'back'}\n",
    "]\n",
    "\n",
    "# Preprocess the training data\n",
    "for item in train_data:\n",
    "    item['text'] = preprocess_text(item['text'])\n",
    "\n",
    "# Extract the features and labels from the training data\n",
    "corpus = [item['text'] for item in train_data]\n",
    "labels = [item['label'] for item in train_data]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Train a Naive Bayes classifier on the training data\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Define a function to listen to user input and make predictions\n",
    "def listen_for_instructions():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input('Please give me an instruction: ')\n",
    "        \n",
    "        # Preprocess the input text\n",
    "        input_text = preprocess_text(input_text)\n",
    "        \n",
    "        # Convert the input text to a feature vector\n",
    "        input_vector = vectorizer.transform([input_text])\n",
    "        \n",
    "        # Make a prediction\n",
    "        prediction = clf.predict(input_vector)\n",
    "        \n",
    "        # Print the prediction\n",
    "        print('I think you said:', prediction[0])\n",
    "\n",
    "# Call the listen_for_instructions() function to start listening for instructions\n",
    "listen_for_instructions()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In this program, we first define a function called preprocess_text() that preprocesses a given text by tokenizing it into words, removing stop words and punctuation, and joining the remaining words back into a string.\n",
    "\n",
    "# We then define a set of training data consisting of instructions and their corresponding labels. We preprocess the training data using the preprocess_text() function and extract the features and labels from it using the CountVectorizer class from scikit-learn.\n",
    "\n",
    "# We train a Naive Bayes classifier on the training data using the MultinomialNB class from scikit-learn.\n",
    "\n",
    "# Finally, we define a function called listen_for_instructions() that listens for user input, preprocesses it using the preprocess_text() function, converts it to a feature vector using the CountVectorizer object, and makes a prediction using the trained classifier. We call this function to start listening for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['launch', 'chrome'], 'open chrome'), (['open', 'chrome', 'browser'], 'open chrome'), (['go', 'to', 'google'], 'go to this website'), (['visit', 'facebook'], 'go to this website'), (['play', 'some', 'music'], 'start music for me'), (['start', 'the', 'playlist'], 'start music for me')]\n",
      "Opening Chrome browser\n",
      "Starting music playback\n",
      "Navigating to specified website\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "# Define the command vocabulary\n",
    "commands = [\"open chrome\", \"go to this website\", \"start music for me\"]\n",
    "\n",
    "# Define training data\n",
    "training_data = [\n",
    "    (\"Launch Chrome\", \"open chrome\"),\n",
    "    (\"Open Chrome browser\", \"open chrome\"),\n",
    "    (\"Go to Google\", \"go to this website\"),\n",
    "    (\"Visit Facebook\", \"go to this website\"),\n",
    "    (\"Play some music\", \"start music for me\"),\n",
    "    (\"Start the playlist\", \"start music for me\")\n",
    "]\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "def preprocess(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    return words\n",
    "\n",
    "# Preprocess the training data\n",
    "preprocessed_data = [(preprocess(sentence), command) for sentence, command in training_data]\n",
    "\n",
    "print(preprocessed_data)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(preprocessed_data)\n",
    "\n",
    "# Extract features using a bag-of-words approach\n",
    "all_words = []\n",
    "for words, _ in preprocessed_data:\n",
    "    all_words.extend(words)\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:100]\n",
    "\n",
    "def extract_features(words):\n",
    "    words = set(words)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features\n",
    "\n",
    "# Extract features for training data\n",
    "training_features = [(extract_features(words), command) for words, command in preprocessed_data]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_features)\n",
    "\n",
    "# Define the response function\n",
    "def respond(sentence):\n",
    "    words = preprocess(sentence)\n",
    "    features = extract_features(words)\n",
    "    command = classifier.classify(features)\n",
    "    if command == \"open chrome\":\n",
    "        return \"Opening Chrome browser\"\n",
    "    elif command == \"go to this website\":\n",
    "        return \"Navigating to specified website\"\n",
    "    elif command == \"start music for me\":\n",
    "        return \"Starting music playback\"\n",
    "    else:\n",
    "        return \"Sorry, I didn't understand that command\"\n",
    "\n",
    "# Test the model\n",
    "print(respond(\"Launch Chrome\"))\n",
    "print(respond(\"Play some music\"))\n",
    "print(respond(\"Go to Google\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3 (threaded_listen):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speech_recognition\\__init__.py\", line 747, in threaded_listen\n",
      "    with source as s:\n",
      "  File \"c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speech_recognition\\__init__.py\", line 186, in __enter__\n",
      "    assert self.stream is None, \"This audio source is already inside a context manager\"\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: This audio source is already inside a context manager\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m r\u001b[39m.\u001b[39mlisten_in_background(source, callback)\n\u001b[0;32m     23\u001b[0m \u001b[39m# keep the program running indefinitely\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# create a recognizer object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# define a callback function to handle the audio data\n",
    "def callback(recognizer, audio):\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)  # use Google Speech Recognition API to recognize the audio\n",
    "        print(\"You said:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Could not request results; {0}\".format(e))\n",
    "\n",
    "# start listening to the microphone input\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Listening...\")\n",
    "    # adjust for ambient noise level\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    # start listening to the microphone with the callback function\n",
    "    r.listen_in_background(source, callback)\n",
    "    # keep the program running indefinitely\n",
    "    while True:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BSTR.__del__ at 0x000002754BD2CA40>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\comtypes\\__init__.py\", line 1043, in __del__\n",
      "    def __del__(self, _free=windll.oleaut32.SysFreeString):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEXT TO SPEECH\n",
    "import pyttsx3\n",
    "\n",
    "# create a TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# set the voice\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[2].id)\n",
    "engine.setProperty('rate', 115) # adjust the speaking rate\n",
    "engine.setProperty('volume', 1)\n",
    "engine.setProperty('gender', 'male')\n",
    "engine.setProperty('age', 10)\n",
    "engine.setProperty('name', 'myEngine')\n",
    "\n",
    "# say the text\n",
    "text = '''I Think I Love You is a song by Tony Romeo'''\n",
    "engine.say(text)\n",
    "\n",
    "# run the TTS engine\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define the training data\n",
    "sentences = [\n",
    "    \"open chrome\",\n",
    "    \"go to this website\",\n",
    "    \"start music for me\",\n",
    "]\n",
    "\n",
    "# define the labels for each command\n",
    "labels = [\n",
    "    \"open_chrome\",\n",
    "    \"go_to_website\",\n",
    "    \"start_music\",\n",
    "]\n",
    "\n",
    "# create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# convert the training data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# pad the sequences so they are all the same length\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "# create the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(padded_sequences, labels, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     40\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model\u001b[39m.\u001b[39;49mfit(padded_sequences, labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ritik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1079\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define the training data\n",
    "sentences = [\n",
    "    \"open chrome\",\n",
    "    \"go to this website\",\n",
    "    \"start music for me\",\n",
    "]\n",
    "\n",
    "# define the labels for each command\n",
    "labels = [\n",
    "    \"open_chrome\",\n",
    "    \"go_to_website\",\n",
    "    \"start_music\",\n",
    "]\n",
    "\n",
    "# create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# convert the training data to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# pad the sequences so they are all the same length\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "# create the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(padded_sequences, labels, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamatar ki sabji\n",
    "# pani puri\n",
    "# khichdi - phariyal\n",
    "# dhokla\n",
    "# back samosa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3615e7d52db689745355dd59d441ef83bf623634b6b7ee4bc44f57c8121b6608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
